\begin{abstract}
% * Summary of the _entire_ report.
% * Must be 150-300 words
% * Puts your work in context, how it was carried out and what its major
%   successes/conclusions were
%    - Not a contents list
%    - Don't use acronyms unless well-understood
% * Must be able to stand entirely on its own

% === [ Keywords ] ===
%
%    * Decompilation
%
%    * Composition
%    * Decompilation pipeline
%    * Stages
%    * Language-agnostic
%
%    * LLVM IR
%    * Control flow primitives
%    * Subgraph isomorphism search
%    * Control flow analysis
%    * Recover / reconstruct
%    * Basic block

% --- [ Background and purpose ] -----------------------------------------------

A dynamic approach to problem solving is the composition of independent and specialized components which communicate using well-defined interfaces. The components may conceptually be arranged in a pipeline of stages which transform, massage or interpret the input in a certain way to solve a given task. This idea is promoted by the Unix philosophy and has had a large influence on software construction. Systems which expose their individual components to end-users facilitate dynamic workflows, as they enable users to modify and extend the system by replacing components and adding further stages respectively.

% TODO: cleanup.

The majority of the production-quality binary analysis tools used for decompilation are monolithic to the end-user, and may at best offer scripting support or plugin extensibility. There exists a gap to fill which learns from successful software construction practices in other problem domains. Specifically the idea of composition.

Composition is a battle-tested method for solving complex problems. Meanwhile the typical arsenal of tools available to the reverse engineer are provided as monolithic beasts, which at best provide scripting and plugin support.

The users of such a system may replace a small component with another to tailor a solution which fits their needs. Meanwhile, if you are a reverse engineer chances are that your artillery of binary analysis tools consists of monolithic decompilers which only provide customization and extensibility through scripting and plugin support. This paper aims to investigate the feasibility of bringing the concept of composition to solve the complex task of decompilation using a pipeline of independent stages which may consist of one or more components written in a variety of programming languages.

The intention is to expose the individual components of the decompilation pipeline to the end-user, who may for each subproblem select between a variety of components with their individual advantages and limitations. Inspiration is drawn from the Unix philosophy of developing small dedicated programs, each dealing with a specific task and solving it well.

Decompilation is the process of transforming low-level machine-readable code into high-level human-readable code. The problem is complex as much information is lost through the process of compilation. The problem can however be broken into several smaller subproblems, which may be solved independently. This paper explores a compositional approach to decompilation, and creates a pipeline of independent components to solve the problems of each stage. A core focus is the language-agnostic aspects of composition, such that the various components may be written in any programming language.

The control flow analysis component of the decompiler pipeline utilizes subgraph isomorphism search algorithms to identify and reconstruct high-level control flow primitives from LLVM IR, which is a platform-independent low-level intermediate representation used by the LLVM compiler framework.

\end{abstract}
