% --- [ Future Work ] ----------------------------------------------------------

\subsection{Future Work}
\label{sec:future_work}

The primary focus for planned future work is to stress test the design of the decompilation pipeline and its individual components. A secondary focus is to improve the quality and the reliability of the components. A tertiary focus is to extend the capabilities of the decompilation pipeline. This prioritization strives to validate the core of the system before extending it.

% ~~~ [ Design Validation ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection{Design Validation}
\label{sec:design_validation}

The principle of separation of concern has influenced every aspect of the design of the decompilation pipeline and its individual components. Conceptually, the components of the decompilation pipeline are grouped into three modules which separate concerns regarding the source language (front-end module), the general decompilation tasks (middle-end module), and the target language (back-end module). This conceptual separation is a vital aspect of the decompilation pipeline design, and it will therefore be thoroughly examined. Should a component violate the principle of separation of concern, either in isolation or within the system as a whole, it must be redesigned or reimplemented. To identify such issues, key areas of the decompilation pipeline will be extended to put pressure on the design.

Firstly, an additional back-end (e.g. support for Python as a target language) will be implemented to put pressure on the design of the middle-end module. The second back-end would only be able to leverage the target-independent information of the general decompilation tasks (e.g. control flow analysis) if the middle-end module was implemented correctly.

Secondly, a key component (e.g. data flow analysis) will be implemented in a separate programming language (e.g. Haskell, Rust, Prolog, …) to validate the language-agnostic aspects of the design. This component would only be able to interact with the rest of the decompilation pipeline, through well-defined input and output (e.g. LLVM IR, JSON, DOT, …), if the other components were implemented correctly.

The separation of the front-end and the middle-end has already been validated. These modules are only interacting through an intermediate representation (i.e. LLVM IR), and a variety of source languages are already supported by the front-end module which consists of components from several independent open source project (e.g. Dagger, Fracture, MC-Semantic, Clang, …).

The design of the control flow analysis component has both advantages and limitations, as discussed in section \ref{sec:design_control_flow_analysis}. The most significant limitation is the lack of support for control flow primitives with a variable number of nodes in their graph representations (e.g. n-way conditionals). To gain a better understanding of this issue, an analysis of control flow primitives from different high-level languages will be conducted. Should the n-node control flow primitives prove to be rare, hard-coded support for n-way conditionals (e.g. \texttt{switch}-statements) and similar control flow primitives would suffice. Otherwise, a general solution to the problem will be required (such as the introduction of a domain specific language which describes dynamic properties of the nodes and edges in DOT files). The OpenCL decompiler presented by S. Moll solved this problem by converting n-way conditionals into sets of 2-way conditionals \cite{decomp_of_llvm}.

As described in section \ref{sec:eval_control_flow_analysis_library}, the current implementation of the control flow analysis component enforces a single-entry/single-exit invariant on the graph representations of high-level control flow primitives. This invariant prevents the recovery of infinite loops, as their graph representation has no exit node. At this stage it is unclear whether a refined implementation may relax this invariant to support single-entry/no-exit graphs, or if the limitation is inherent to the design. The issue will be further investigated to gain a deeper understanding.

% ~~~ [ Reliability Improvements ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection{Reliability Improvements}

As described in section \ref{sec:go_bindings_for_llvm}, there are many reliability issues caused by the Go bindings for LLVM. To mitigate these issues a pure Go library is being developed for interacting with LLVM IR (see section \ref{sec:llvm_ir_library}). This library will be reusable by other projects, and the requirements of the third-party Go compiler \texttt{llgo}\footnote{LLVM-based compiler for Go: \url{https://llvm.org/svn/llvm-project/llgo/trunk/README.TXT}} are actively being tracked\footnote{Requirements · Issue \#3: \url{https://github.com/llir/llvm/issues/3}}.

To ensure reliable interoperability between components written in different programming languages, the intermediate representation (i.e. LLVM IR) of the decompilation pipeline must be well-defined. Previous efforts to produce a formal grammar for LLVM IR have only focused on subsets of the language (as mentioned in section \ref{sec:req_llvm_ir_library}), and no such grammar has been officially endorsed. Producing an official formal specification for LLVM IR would require huge efforts, but it would enable interesting opportunities. For instance, with a formal grammar it would be possible to create a tool which automatically generates gramatically correct LLVM IR assembly which may be used to verify the various implementation of LLVM IR. This approach has been used by the GoSmith tool to generate random, but legal, Go programs which have uncovered 31 bugs in the official Go compiler, 18 bugs in the Gccgo compiler, 5 bugs in the \texttt{llgo} compiler, and 3 bugs in the Go language specification \cite{gosmith}.

% ~~~ [ Extended Capabilities ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection{Extended Capabilities}

The official Go compiler was automatically translated from C to Go in preparation for the 1.5 release (to be released in August 2015) \cite{go_compiler_c2go}. To make the C-style Go source code more idiomatic, Russ Cox wrote a tool called \texttt{grind}\footnote{Grind polishes Go programs: \url{https://github.com/rsc/grind}} which moves variable declarations closer to their usage. This tool is a perfect fit for the decompilation pipeline, and may be used as-is to extend the post-processing stage of the Go back-end.

In the far future, a type analysis component will be implement to support type recovery during decompilation. As type analysis requires type constraints equations to be solved, the component will be implemented in a language with good support support for constraints programming (e.g. Prolog). At this stage, more research is required to determine how generic type inference algorithms (e.g. Algorithm W \cite{algorithm_w}) may influence the design.
