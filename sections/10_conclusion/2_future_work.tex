% --- [ Future Work ] ----------------------------------------------------------

\subsection{Future Work}
\label{sec:future_work}

The primary focus for planned future work is to stress test the design of the decompilation pipeline and its individual components. A secondary focus is to improve the quality and the reliability of the components. A tertiary focus is to extend the capabilities of the decompilation pipeline. This prioritization strives to validate the core of the system before extending it.

% ~~~ [ Design Validation ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection{Design Validation}
\label{sec:design_validation}

The principle of separation of concern has influenced every aspect of the design of the decompilation pipeline and its individual components. Conceptually, the components of the decompilation pipeline are grouped into three modules which separate concerns regarding the source language (front-end module), the general decompilation tasks (middle-end module), and the target language (back-end module). This conceptual separation is a vital aspect of the decompilation pipeline design, and it will therefore be thoroughly examined. Should a component violate the principle of separation of concern, either in isolation or within the system as a whole, it must be redesigned or reimplemented. To identify such issues, key areas of the decompilation pipeline will be extended to put pressure on the design.

Firstly, an additional back-end (e.g. support for Python as a target language) will be implemented to put pressure on the design of the middle-end module. The second back-end would only be able to leverage the target-independent information of the general decompilation tasks (e.g. control flow analysis) if the middle-end module was implemented correctly.

Secondly, a key component (e.g. data flow analysis) will be implemented in a separate programming language (e.g. Haskell, Rust, Prolog, …) to validate the language-agnostic aspects of the design. This component would only be able to interact with the rest of the decompilation pipeline, through well-defined input and output (e.g. LLVM IR, JSON, DOT, …), if the other components were implemented correctly.

The separation of the front-end and middle-end has already been validated. These modules are only interacting through an intermediate representation (i.e. LLVM IR), and a variety of source languages are already supported by the front-end module which consists of components from several independent open source project (e.g. Dagger, Fracture, MC-Semantic, Clang, …).

% ~~~ [ Reliability Improvements ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection{Reliability Improvements}

As described in section \ref{sec:go_bindings_for_llvm}, there are many reliability issues caused by the Go bindings for LLVM. To mitigate these issues a pure Go library is being developed for interacting with LLVM IR (see section \ref{sec:llvm_ir_library}). This library will be reusable by other projects, and the requirements of the third-party Go compiler \texttt{llgo}\footnote{LLVM-based compiler for Go: \url{https://llvm.org/svn/llvm-project/llgo/trunk/README.TXT}} are actively being tracked\footnote{Requirements · Issue \#3: \url{https://github.com/llir/llvm/issues/3}}.

To ensure reliable interoperability between components written in different programming languages, the intermediate representation (i.e. LLVM IR) of the decompilation pipeline must be well-defined. Previous efforts to produce a formal grammar for LLVM IR have only focused on subsets of the language (as mentioned in section \ref{sec:req_llvm_ir_library}), and no such grammar has been officially endorsed. Producing an officially accepted formal specification of LLVM IR would require huge efforts, but it would enable interesting opportunities. For instance, with a formal grammar it would be possible to create a tool which automatically generates gramatically correct LLVM IR assembly which may be used to verify the various implementation of LLVM IR. This approach has been used by the GoSmith tool to generate random, but legal, Go programs which has uncovered 31 bugs in the official Go compiler, 18 bugs in the Gccgo compiler, 5 bugs in the \texttt{llgo} compiler, and 3 bugs in the language specification \cite{gosmith}.

% TODO: Investigate various ways to mitigate the limitations of the control flow analysis design.

% The data-driven design of the control flow analysis component has both advantages and limitations.

% It facilitates extensibility, as support for new control flow primitives may be added by describing their control flow with a directed graph

% ~~~ [ Extended Capabilities ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection{Extended Capabilities}

% Several extensions to the decompilation pipeline are planned, apart from the second back-end and the data flow analysis component already described in section \ref{sec:design_validation}.

% In programs written in C

% The post-processing stage of the Go back-end can easily be extended to make the output more idiomatic.

% The output of the Go back-end will be made mode idiomatic by introducing further post-processing stages.

% Further post-processing stages will be added to the Go back-end to make the output more idiomatic.

% TODO: Add grind (add footnote to rsc's grind tool)

In the far future, a type analysis component will be implement to support type recovery during decompilation. As type analysis requires type constraints equations to be solved, the component will be implemented in a language with good support support for constraints programming (e.g. Prolog). At this stage, more research is required to determine how generic type inference algorithms (e.g. Algorithm W \cite{algorithm_w}) may influence the design.
